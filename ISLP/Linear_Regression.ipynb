{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c0ef4f",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828ea0b",
   "metadata": {},
   "source": [
    "This chapter is about linear regression, a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. It has been around for a long time and is the topic of innumerable textbooks. Though it may seen somewhat dull compared to some of the more mordern statistical learning approaches described in later chapters of this book, linear regression is still a useful and widely used statistical learning method. Moreover, it servers as a good jumping-off point for newer approaches: as we will see in later chapters, many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated. In this chapter, we review some of the key ideas underlying the linear regression model, as well as the least squares approach that is most commonly used to fit this model.\n",
    "Recall the Advertising data from Chapter 2. Figure 2.1 displays sales(in thousands of units) for a particular product as a function of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? Here are a few important questions that we might seek to address:\n",
    "\n",
    "1. Is there a relationship between advertising budget and sales?\n",
    "    Our first goal should be to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, the one might argue that no money should be spent on advertising!\n",
    "\n",
    "2. How strong is the relationship between advertising budget and sales?\n",
    "    Assuming that there is a relationship between advertising and sales, we would like to know the strength of this relationship. Does knowledge of the advertising budget provide a lot of information about product sales?\n",
    "\n",
    "3. Which media are associated with sales?\n",
    "    Are all three media-TV, radio, and newspaper-associated with sales, or are just one or two of the media associated? To answer this question, we must find a way to separate out the individual contribution of each medium to sales when we have spent money on all three media.\n",
    "\n",
    "4. How large is the association between each medium and sales?\n",
    "    For every dollar spent on advertising in particular medium, by what amount will sales increase? How accurately can we predict this amount of increase?\n",
    "\n",
    "5. How accurately can we predict future sales?\n",
    "    For any given level of television, radio or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\n",
    "\n",
    "6. Is the relationship linear?\n",
    "    If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used.\n",
    "\n",
    "7. Is there synergy among the advertising media?\n",
    "    Perhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect, while in statistics it is called an interaction effect.\n",
    "\n",
    "It turns out that linear regression can be used to answer each of these questions. We will first discuss all of these questions in a general context, and then return to them in this specific context in Section 3.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796f2cc",
   "metadata": {},
   "source": [
    "3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ee797",
   "metadata": {},
   "source": [
    "Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as \n",
    "Y ≈ β0 + β1X.\n",
    "\n",
    "You might read \"≈\" as \"is approximately modeled as\". We will sometimes describe (3.1) by saying that we are regressing Y on X (or Y onto X).\n",
    "\n",
    "For example, X may represent TV advertising and Y may represent sales.\n",
    "Then we can regress sales onto TV by fitting the model.\n",
    "\n",
    "sales ≈ β0 + β1 × TV.\n",
    "\n",
    "In Equation 3.1,  β0 and β1 are two unknown constants that represent the intercept and slope terms in the linear model. Together, β0 and β1 are known as the model coefficients or parameters. Once we have used our training data to produce estimates βˆ0 and βˆ1 for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing\n",
    "\n",
    "yˆ = βˆ0 + βˆ1x,\n",
    "\n",
    "where y^ indicates a prediction of Y on the basis of X = x. Here we use a hat symbol, ^, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47907ae4",
   "metadata": {},
   "source": [
    "3.1.1 Estimating the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ab732",
   "metadata": {},
   "source": [
    "In practice, β0 and β1 are unknown. So before we can use (3.1) to make predictions, we must use data to estimate the coefficients. Let\n",
    "    (x1, y1), (x2, y2),..., (xn, yn)\n",
    "represent n observation pairs, each of which consists of a measurement of X and a measurement of Y. In the Advertising example, this data set consists of the TV advertising budget and product sales in n = 200 different markets. (Recall that the data are displayed in Figure 2.1.) Our goal is to obtain coefficinet estimates βˆ0 and βˆ1 such that the linear model (3.1) fits the available data well, that is, so that yi ≈ βˆ0 + βˆ1xi for i = 1,...,n. In other words, we want to find an intercept βˆ0 and a slope βˆ1 such that the resulting line is as close as possible to the n = 200 data points. There are a number of ways of measuring closeness. However, by far the most common approach involves minimizing the least squares criterion, and we take that approach in this chapter. Alternative approaches will be considered in Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca15921",
   "metadata": {},
   "source": [
    "3.1.2 Assessing the Accuracy of the Coefficient Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117aa292",
   "metadata": {},
   "source": [
    "Recall from (2.1) that we assume that the true relationship between X and Y takes the form Y = f(X) + e for some unknown function f, where e is a mean-zero random term. If f is to be approximated by a linear function, then we can write this relationship as Y = β0 + β1X + \".\n",
    "Here β0 is the intercept term, that is, the expected value of Y when X = 0, and β1 is the slope, the average increase in Y associated wit a one-unit increase in X. The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in Y, and there may be measurement error. We typically assume that the error term is independent of X.\n",
    "The model given by (3.5) defines the population regression line, which is the best linear approximation to the true relationship between X and Y. The least squares regression coefficent estimates (3.4) characterize the least squares line (3.2). The left-hand panel of Figure 3.3 displays these two lines in a simple simulated example. We created 100 random Xs, and generated 100 corresponding Ys from the model Y = 2 + 3X + e, where e was generated from a normal distribuition with mean zero. The red line in the left-hand panel of Figure 3.3 displays the true relationship, f(X) = 2 + 3X, while the blue line is the least squares estimate based on the observer data. The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficinet estimates given in (3.4). In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved. In the right-hand panel of Figure 3.3 we have generated ten different data sets from the model given by (3.6) and plotted the corresponding ten least squares lines. Notice that different data sets generated from the same true model result in slightly different least square lines, but the unobserved population regression line does not change.\n",
    "At first glance, the difference between the population regression line and the least squares line may seem subtle and confusing. We only have one data set, and so what does it mean that two different lines describe the relationship between the predictor and the response? Fundamentally, the concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population. For example, suppose that we are interest in knowing the population mean u of some random variable Y. Unfortunately, u is unknown, but we do have access to n observations from Y, y1,...,yn, which we can use to estimate u. A reasonble estimate is û = ^y, where ^y = 1/n(Sum(i=1, n))yi is the sample mean. The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean. In the same way, the unknown coefficients β0 and β1 in linear regression define the population regression line. We seek to estimate these unknown coefficients using βˆ0 and βˆ1 given in (3.4). These coefficient estimates define the least square line.\n",
    "The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias. If we use the sample mean û to estimate u, this estimate is unbiased, in the sense that on average, we expect û to equal u. What exactly does this mean? It means that on the basis of one particular set of observations y1,...yn, û might overestimate u, and on the basis of another set of observations, û might underestimate u. But if we could average a huge number of estimates of u obtained from a huge number of sets of observations, then this average would exactly equal u. Hence, an unbiased estimator does not systematically over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates given by (3.4) as well: if we estimate β0 and β1 on the basis of a particular data set, then our estimates won't be exactly equal to β0 and β1. But if we could average the estimates obtained over a huge number of data sets, the the average of these estimates would be spot on! In fact, we can see from the right-hand panel of Figure 3.3 that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line. We continue the analogy with the estimation of the population mean u of a random variable Y. A natural question is as follows: how accurate is the sample mean û as an estimate of u? We have established that the average of û's over many data sets will be very close to u, but that a single estimate û may be a substantial underestimate or overestimate of u. How far off will that single estimate of û be? In general, we answer this question by computing the standard error of û, written as SE(û). We have the well-known formula Var(û) = SE(û)^2 = σ2/n , where σ(Sigma) is the standard deviation of each of the realizations yi of Y. Roughly speaking, the standard error tells us the average amount that this estimate û differs from the actual value of u."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
